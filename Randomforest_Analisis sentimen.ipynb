{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d3be563-7495-40d1-93f1-c94f620f8c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library yang diperlukan\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1c70970-fdff-447f-99d3-9aafe6c188e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ulasan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>terima kasih bonus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kemasan baru menarik coba  enak lembut kotoran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bagus poko recommended keseharian kerja double...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bner mengangkat kotoran wajah tanpa membuat ke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lembut man</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              ulasan\n",
       "0                                 terima kasih bonus\n",
       "1  kemasan baru menarik coba  enak lembut kotoran...\n",
       "2  bagus poko recommended keseharian kerja double...\n",
       "3  bner mengangkat kotoran wajah tanpa membuat ke...\n",
       "4                                         lembut man"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Membaca data\n",
    "df = pd.read_excel(\"C:/Users/hp/Downloads/Semester 5/Scrape Sinergia/Shopee/clean ulasan face up saja shopee.xlsx\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b3e959a-be6d-484a-8805-b2ebbaa6494f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names in the dataset: Index(['ulasan'], dtype='object')\n",
      "Generating sentiment labels...\n",
      "Preprocessing text...\n",
      "Handling class imbalance with SMOTE...\n",
      "Training model...\n",
      "\n",
      "Classification Report (Training):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00       492\n",
      "     neutral       1.00      1.00      1.00       486\n",
      "    positive       1.00      1.00      1.00       486\n",
      "\n",
      "    accuracy                           1.00      1464\n",
      "   macro avg       1.00      1.00      1.00      1464\n",
      "weighted avg       1.00      1.00      1.00      1464\n",
      "\n",
      "\n",
      "Classification Report (Testing):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00       159\n",
      "     neutral       0.93      1.00      0.96       165\n",
      "    positive       1.00      0.93      0.96       165\n",
      "\n",
      "    accuracy                           0.98       489\n",
      "   macro avg       0.98      0.98      0.98       489\n",
      "weighted avg       0.98      0.98      0.98       489\n",
      "\n",
      "\n",
      "Confusion Matrix (Testing):\n",
      "[[159   0   0]\n",
      " [  0 165   0]\n",
      " [  0  12 153]]\n",
      "Performance Metrics (Testing):\n",
      "Accuracy: 0.9754601226993865\n",
      "Precision: 0.9771238431943433\n",
      "Recall: 0.9754601226993865\n",
      "F1 Score: 0.9754276303097135\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Download stopwords (run this once to ensure NLTK has the stopwords dataset)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text, lang='indonesian'):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Tokenize and remove stopwords\n",
    "    try:\n",
    "        stop_words = set(stopwords.words(lang))\n",
    "    except OSError:\n",
    "        print(f\"Warning: '{lang}' stopwords not found. Defaulting to English.\")\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    # Join the words back into a single string\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Function to generate sentiment labels using TextBlob\n",
    "def generate_sentiment_labels(text):\n",
    "    blob = TextBlob(text)\n",
    "    # Classify sentiment as positive, negative, or neutral\n",
    "    if blob.sentiment.polarity > 0:\n",
    "        return 'positive'\n",
    "    elif blob.sentiment.polarity < 0:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "# Function to load data, preprocess, and train the model\n",
    "def train_and_evaluate_with_grid_search(file_path):\n",
    "    # Load dataset\n",
    "    df = pd.read_excel(file_path)  # Change to pd.read_excel for Excel files\n",
    "    \n",
    "    # Print column names to check if 'ulasan' exists\n",
    "    print(\"Column names in the dataset:\", df.columns)\n",
    "\n",
    "    # Check if 'ulasan' column exists\n",
    "    if 'ulasan' not in df.columns:\n",
    "        print(\"Error: 'ulasan' column not found in the dataset. Please check the column names.\")\n",
    "        return\n",
    "\n",
    "    # Handle missing values in 'ulasan' column\n",
    "    df.dropna(subset=['ulasan'], inplace=True)\n",
    "\n",
    "    # Generate sentiment labels based on 'ulasan' column\n",
    "    print(\"Generating sentiment labels...\")\n",
    "    df['sentiment'] = df['ulasan'].apply(generate_sentiment_labels)\n",
    "\n",
    "    # Preprocess the text data\n",
    "    print(\"Preprocessing text...\")\n",
    "    df['ulasan'] = df['ulasan'].apply(lambda x: preprocess_text(x, lang='indonesian'))\n",
    "\n",
    "    # Split dataset into features and labels\n",
    "    X = df['ulasan']\n",
    "    y = df['sentiment']\n",
    "\n",
    "    # Vectorize the text using TF-IDF\n",
    "    vectorizer = TfidfVectorizer(max_features=5000)\n",
    "    X = vectorizer.fit_transform(X)\n",
    "\n",
    "    # Handle class imbalance with SMOTE\n",
    "    print(\"Handling class imbalance with SMOTE...\")\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.25, random_state=42)\n",
    "\n",
    "    # Initialize classifier\n",
    "    clf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "    # Train the model\n",
    "    print(\"Training model...\")\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate on training data\n",
    "    y_train_pred = clf.predict(X_train)\n",
    "    print(\"\\nClassification Report (Training):\")\n",
    "    print(classification_report(y_train, y_train_pred))\n",
    "\n",
    "    # Evaluate on testing data\n",
    "    y_test_pred = clf.predict(X_test)\n",
    "    print(\"\\nClassification Report (Testing):\")\n",
    "    print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "    # Confusion Matrix\n",
    "    print(\"\\nConfusion Matrix (Testing):\")\n",
    "    print(confusion_matrix(y_test, y_test_pred))\n",
    "\n",
    "    # Calculate additional metrics\n",
    "    print(\"Performance Metrics (Testing):\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_test_pred)}\")\n",
    "    print(f\"Precision: {precision_score(y_test, y_test_pred, average='weighted')}\")\n",
    "    print(f\"Recall: {recall_score(y_test, y_test_pred, average='weighted')}\")\n",
    "    print(f\"F1 Score: {f1_score(y_test, y_test_pred, average='weighted')}\")\n",
    "\n",
    "# File path to the dataset\n",
    "file_path = \"C:/Users/hp/Downloads/Semester 5/Scrape Sinergia/Shopee/clean ulasan face up saja shopee.xlsx\"\n",
    "\n",
    "# Run the function\n",
    "train_and_evaluate_with_grid_search(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5a7eb3d-e5de-40aa-8436-f614702e1bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "  Downloading textblob-0.18.0.post0-py3-none-any.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: nltk>=3.8 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from textblob) (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk>=3.8->textblob) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk>=3.8->textblob) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk>=3.8->textblob) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk>=3.8->textblob) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\anaconda3\\lib\\site-packages (from click->nltk>=3.8->textblob) (0.4.6)\n",
      "Downloading textblob-0.18.0.post0-py3-none-any.whl (626 kB)\n",
      "   ---------------------------------------- 0.0/626.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 626.3/626.3 kB 7.7 MB/s eta 0:00:00\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.18.0.post0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ (C:\\Users\\hp\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ (C:\\Users\\hp\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ (C:\\Users\\hp\\anaconda3\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83318f58-3701-4a0a-9e18-69a91b96fa04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\hp\\anaconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ (C:\\Users\\hp\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ (C:\\Users\\hp\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ (C:\\Users\\hp\\anaconda3\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "654364bc-c19c-47e7-aa34-fae01036ee4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imbalanced-learn in c:\\users\\hp\\anaconda3\\lib\\site-packages (0.12.3)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.5.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from imbalanced-learn) (3.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ (C:\\Users\\hp\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ (C:\\Users\\hp\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ (C:\\Users\\hp\\anaconda3\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install imbalanced-learn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
